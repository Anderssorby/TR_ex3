---
title: "Analysis of ARMA(2,1) process"
subtitle: "TMA4285 Time series models - Exercise 3"
author: Anders Christiansen Sørby, Edvard Hove
#bibliography: mybiblio.bib
header-includes:
  - \usepackage{dsfont}
  - \DeclareMathOperator*{\E}{\mathbb{E}}
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Abstract
Blabla

# Introduction

## The ARMA process
A stationary process \(\{X_t\}\) is an \(ARMA(p,q)\) process if the following relation exists

\begin{equation}
\phi(B)X_t = \theta(B)Z_t
\label{eq:arma}
\end{equation}

where \(Z_t \sim \bf{WN}(0, \sigma^2)\), and \(\phi(\cdot)\) and \(\theta(\cdot)\) are polynomials (with no common factors) given by

\begin{equation}
\phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p
\end{equation}

and
\begin{equation}
\theta(z) = 1 + \theta_1 z + \theta_2 z^2 +  \dots + \theta_q z^q,
\end{equation}

and B is the backward shift operator. That is \(B^j X_t = X_{t-j}, j \in \mathds{Z}\).

In particular an \(ARMA(2,1)\) process is given by
$$
X_t -\phi_1 X_{t-1} - \phi_2 X_{t-2} = Z_t + \theta_1 Z_{t-1}
$$

# Theory
An expression for \(\{X_t\}\) satisfying equation \eqref{eq:arma} on the form

\begin{equation}
X_t = \sum_{j = -\infty}^\infty \psi_j Z_{t-j}
\end{equation}

is said to be a stationary solution. A stationary solution exists, and is also unique, if no roots of the autoregressive polynomial \(\phi(z)\) lie on the unit circle. If \(X_t\) can be expressed in terms of \(Z_s,s \leq t\) we say that the process is causal. Such an expression is guaranteed to exist if no roots of \(\phi(z)\) lie on the unit disk.
Include how the \(\phi \theta\) is estimated

## Model parameter estimation
The \(ARMA(2,1)\) process (with zero mean) consists of 4 parameters that need to be estimated. That is \(\phi_1, \phi_2, \theta_1 \text{ and } \sigma^2\). This can be done in a number of ways. A simple and consistent way is the method of moments. The idea is to take the expected value of the expression of \(X_t\) multiplied with \(X_{t-h}\).

\begin{equation}
{\E}[X_t X_{t-h}] = {\E}[(Z_t + \theta_1 Z_{t-1} + \phi_1 X_{t-1} + \phi_2 X_{t-2})X_{t-h}]
\end{equation}

This then gives a set of equations containing the autocovariance function  \(\gamma(h)\) and the parameters. By using the sample autocovariance function \(\hat \gamma(h)\) in place of the autocovariance function we can solve the equations, resulting in estimates of the parameters of interest.

The estimators found using this method may be biased and they often have high variance compared to other estimators. They are however useful as first approximations in numerical methods of estimation.

Another way is to use the maximum likelihood estimators.


REPEATED ESTIMATION ON SIMULATED DATA COMES HERE:
IN ORDER TO USE arima.sim WE NEED AN EXPRESSION FOR THE INNOVATIONS

We need to confirm that a given process is indeed an $ARMA(2,1)$


```{r read, echo=FALSE}
x = read.table("DataEx3G8.txt")
```
# Data analysis
The language R has many built-in functions for analysing timeseries. 

Our recieved realization of the process can be seen in the next plot.

```{r TSMplot, echo=FALSE}
plot(seq(1,500),x[,1],type="l", xlab="t", ylab="X_t")
```

There is no obvious need for a transformation of the data to fit an ARMA model. Based on the plot it seems probable that both the mean and variance is constant with respect to time. (DETTE HADDE VÆRT FINT Å DEMONSTRERE)

```{r lagplot, echo=FALSE}
lag.plot(x[,1], 9, diag.col = "red")
```

```{r arma}
arima=arima(x,order=c(2,0,1))

phi1 <- arima$coef[1]
phi2 <- arima$coef[2]
theta1 <- arima$coef[3]
```
```{r acvf}
lag.max=100
acf=acf(x,lag.max=lag.max)
lines(seq(0,lag.max),ARMAacf(ar=c(0.7411,-0.1282),ma=0.2789,lag.max=lag.max))
n=seq(0,25)
s=0.1*sin(2*pi*(n-6)/14)
lines(n,s)
```
```{r transform}
p=10
c=2*pi/p
y=asin(c*x[,1])
```

It is conceivable, based on the sample autocorrelation function, that there is some underlying seasonal lag. More importantly the sample ACF forces us to reject the hypothesis that the data is generated by an \(MA(q)\) model (atleast for \(q\leq 50\)).

```{r armasim}
xsim <- arima.sim(n=500, list(ar=c(arima$coef[1],arima$coef[2]), ma=arima$coef[3]),innov=rnorm(1000, sd=sqrt(arima$sigma2)),start.innov=rnorm(100, sd=sqrt(arima$sigma2)))
plot(xsim)
```

Using the function `arima` we get MLE parameter estimates for our the underlying distribution. 
```{r arimamodel}
print(arima)
```

We can then check the causality criterion by solving
\begin{equation}
\phi(z) = 1 - `r sprintf('%.3f', arima$coef[1])`z + `r sprintf('%.3f', abs(arima$coef[2]))`z^2 =0,
\end{equation}

```{r roots, include=FALSE}
z_1=(arima$coef[1]-sqrt(arima$coef[1]^2 +4*arima$coef[2]))/(-2*arima$coef[2])
z_2=(arima$coef[1]+sqrt(arima$coef[1]^2 +4*arima$coef[2]))/(-2*arima$coef[2])
```

which gives us the roots \(z_1 \approx `r sprintf('%.3f', z_1)`\) and \(z_2 \approx `r sprintf('%.3f', z_2)`\). Since both these roots are outside the unit disk there exists an expression for \(X_t\) on the form

\begin{equation}
X_t = \sum _{i=0}^\infty \psi_{i}Z_{t-i}
\end{equation}

## Model prediction

Given the two preciding values and our estimated \(Z_t\sim\bf{WN}(0, \sigma^2)\)-values we can predict the next value of the time series and compare them to the actual value.

```{r prediction}
source('Zestimates.R')
pred <- rep(0, 497)
residual <- rep(0,497)
for (i in seq(3,500)) {
  pred[i-2] <- phi1*x[,i-1] + phi2*x[,i-2] + Z[i] + theta1*Z[i-1]
  residual[i-2] <- x[,i] - pred[i-2]
}

```

# Discussion

Bla bla


# Conclusion

This was ..

# Appendix

Everything not in the main section
This also includes R-code as a whole

# References

